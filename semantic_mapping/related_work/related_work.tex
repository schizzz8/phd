\documentclass[]{article}

\begin{document}
\section{Related Work}
Recovering the geometric model of a scene from visual and/or range sensors is a well-known problem that spans across different research domains: in the Computer Vision community it goes under the name of "Structure from Motion" (SfM), while in Computer Graphics it's referred to as "Surface Reconstruction", just to name a few examples. In Robotics, this is a relevant problem in the context of mobile robots that operate in unknown environments, where they have to build a representation of their surroundings to localize themselves in the environment and plan motion to complete their tasks. In this context, the geometric reconstruction is mainly performed by addressing the "Simultaneous Localization and Mapping" (SLAM) problem, since it allows to track the robot trajectory during the exploration and "in parallel" to build a metric representation of the environment for the above-mentioned goals. For 3D geometry, different types of representation have been proposed and, according to \cite{cadena2016past}, can be categorized by the way in which they describe the objects surface.

In the majority of SLAM methods, the scene is represented by a set of sparse 3D landmarks that correspond to relevant features in the environment (e.g. edges, corners) \cite{mur2015orb,klein2007parallel}. These methods are attractive because they're based on a lightweight formulation and still they're effective in accurately localizing the robot, but they provide a rather poor representation of the environment since they tend to filter out portions with less features.

A different approach is to describe the 3D geometry by means of an unstructured set of points (i.e., point clouds), which can be obtained by nowadays easily available sensors like RGB-D cameras. Opposed to landmark-based methods, there are also "direct" methods that estimate the trajectory of the robot and a 3D model directly from the intensity and/or depth values of all the image pixels \cite{newcombe2011dtam,kerl13iros}. This type of representation allows to better infer the structure of the scene but it may contain holes in some portions of the environment due to the uneven sensor sampling density.

A reasonable choice to overcome this limitation is to introduce structured representations, such as the ones used in Computer Graphics for Surface Reconstruction \cite{berger2014state}. These are called "boundary representations", since they define 3D objects in terms of their surface boundary. In general, b-reps can be classified as: curve-based representations (NURBS or B-Spline), mesh representations (triangulated surfaces) and implicit representations (level-set surfaces). The latter was investigated in the work of Newcombe et al. \cite{newcombe2011kinectfusion} and received big interest from the SLAM community thanks to its power of representation. The main drawback is the not negligible amount of memory required also in the case of small scale scenes. This limits considerably the scalability of this representation and the development of optimized techniques for large-scale environments is an hot topic in research \cite{kahler2015very,steinbrucker2014volumetric}.

Despite all the representations mentioned so far can be considered as a good approximation to the scene geometry, they all suffer from a structural problem which is common to most robotic applications, i.e., occlusions. The natural way to overcome this problem, would be to frame the object to be reconstructed from as many points of view as possible. Of course, for a robot, this is not always feasible and leads to incomplete reconstruction of objects surfaces. The solution proposed in the seminal work by Moreno et al. \cite{salas2013slam++} is to investigate higher level object-based representations, where the map is made of objects and solid shapes as it is the case in Computer Aided Design (CAD). Of course, building such a map is an expensive procedure and requires many simplifying assumptions. Nevertheless, still this type of representation is very attractive because it naturally lends itself to represent also objects semantic and physical properties, which can be very important for the robot to execute its tasks.

\bibliographystyle{plain}
\bibliography{reference}

\end{document}
